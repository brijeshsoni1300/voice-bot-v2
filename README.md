# Voice Bot Library

A Python library that enables voice interactions with LLM chatbots by combining Speech-to-Text (Deepgram), LLM processing (OpenAI), and Text-to-Speech (OpenAI TTS).

## Core Features

- **Speech-to-Text**
  - Real-time audio capture from microphone
  - Streaming transcription via Deepgram
  - Final transcript delivery when user stops speaking

- **LLM Processing**
  - Processes transcribed text through OpenAI's API
  - Generates text responses using specified LLM model
  - Configurable system prompts and parameters

- **Text-to-Speech**
  - Converts LLM responses to speech using OpenAI's TTS
  - Streams audio output to speakers
  - Built-in pause/resume to prevent audio feedback

- **Performance Monitoring**
  Tracks key latency metrics:

  | Metric | Description | IMPL Status |
  |--------|-------------|---------|
  | STT Latency | Time from speech end to transcript completion | Pending |
  | LLM First Token | Time to first token from LLM | Pending |
  | LLM Full Response | Total time for complete LLM response | Implemented |
  | End-to-End Latency | Time from speech end to first TTS output | Pending |

## Prerequisites

- Python 3.10 or higher
- API Keys:
  - Deepgram API key for Speech-to-Text
  - OpenAI API key for LLM and TTS
- Working microphone and speakers
- Operating System: Windows, macOS, or Linux

## Quick Start

1. **Clone the Repository**
   ```bash
   git clone https://github.com/username/voice-bot-v2.git
   cd voice-bot-v2
   ```

2. **Install Dependency**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the Bot**
   ```bash
   python cli.py --dp-key DEEPGRAM_API_KEY --ooi-key OPENAI_API_KEY
   ```

## How It Works

1. Captures audio from microphone and streams to Deepgram
2. Receives transcribed text when user stops speaking
3. Sends transcript to OpenAI's LLM for processing
4. Converts LLM response to speech using OpenAI's TTS
5. Plays audio response through speakers
6. Ends conversation when user says "goodbye"

## Project Structure

```
voice_bot/
├── adapters/                # External API integrations
│   ├── stt_deepgram.py     # Deepgram STT client
│   ├── llm_openai.py       # OpenAI LLM client
│   └── tts_openai.py       # OpenAI TTS client
├── streams/                 # Audio handling
│   ├── input_stream.py     # Microphone input
│   └── output_stream.py    # Speaker output
├── config.py               # Configuration
├── conversation.py         # Core logic
├── metrics.py             # Performance tracking
└── cli.py                 # Command-line interface
```


## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit your changes (`git commit -m 'Add new feature'`)
4. Push to the branch (`git push origin feature/new-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Support

- Issue Tracker: [Link to issues]
- Documentation: [Link to docs]

---

Built by [Your Name/Organization]

## Potential Latency Improvements 

### Streamed LLM Response for TTS

- **Current Implementation:**  
  In the current design, the system waits for the OpenAI LLM to complete generating the full response before passing the complete text to the TTS service. This sequential process introduces extra delay since the TTS conversion only starts after the entire response is ready.

- **Proposed Improvement:**  
  By modifying the LLM processing module to **stream** its response tokens as they are generated, each token can be immediately sent to the TTS service. This overlapping of processing tasks allows the TTS to start producing audio as soon as the first token arrives, rather than waiting for the complete response. As a result, the overall latency is reduced, and the user experiences a more immediate and continuous audio response.

**Integration Flow Diagram:**

```
     +-------------------+          
     | Microphone Input  |          
     +-------------------+          
               |                  
               v                  
      +-----------------+          
      |  Deepgram STT   |  <-- Streaming Transcription
      +-----------------+          
               |                  
               v                  
      +----------------------+       
      |   OpenAI LLM         |  <-- Streaming LLM Response
      |  (Streaming Enabled) |       
      +----------+-----------+       
                 |                  
                 v                  
      +----------------------+       
      |   OpenAI TTS         |  <-- Streaming Audio Output
      |   (Real-time TTS)    |       
      +----------+-----------+       
                 |                  
                 v                  
             Speakers                
```

*With this approach, the moment the first token is generated by the LLM, it is piped directly to the TTS service. Even as the LLM continues processing the remainder of the response, TTS starts converting available tokens to speech, thus considerably reducing the end-to-end latency.*
